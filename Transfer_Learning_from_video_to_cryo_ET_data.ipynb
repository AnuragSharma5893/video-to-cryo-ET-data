{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOY1nEMhs2lx1vbc3SlPy0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuragSharma5893/video-to-cryo-ET-data/blob/main/Transfer_Learning_from_video_to_cryo_ET_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt_CZIfeQr7d"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Installations\n",
        "!pip install -q mrcfile transformers huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Analysis Pipeline Code\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import mrcfile\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.ndimage import zoom\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- 3D-ResNet34 Model Definition (from kenshohara/video-classification-3d-cnn-pytorch) ---\n",
        "def conv3x3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def conv1x1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv3d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, block_inplanes, n_input_channels=3, conv1_t_size=7,\n",
        "                 conv1_t_stride=1, no_max_pool=False, shortcut_type='B', widen_factor=1.0, n_classes=400):\n",
        "        super().__init__()\n",
        "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
        "        self.in_planes = block_inplanes[0]\n",
        "        self.no_max_pool = no_max_pool\n",
        "        self.conv1 = nn.Conv3d(n_input_channels, self.in_planes, kernel_size=(conv1_t_size, 7, 7),\n",
        "                               stride=(conv1_t_stride, 2, 2), padding=(conv1_t_size // 2, 3, 3), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0], shortcut_type)\n",
        "        self.layer2 = self._make_layer(block, block_inplanes[1], layers[1], shortcut_type, stride=2)\n",
        "        self.layer3 = self._make_layer(block, block_inplanes[2], layers[2], shortcut_type, stride=2)\n",
        "        self.layer4 = self._make_layer(block, block_inplanes[3], layers[3], shortcut_type, stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, 1000)  # Change n_classes to 1000\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _downsample_basic_block(self, x, planes, stride):\n",
        "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2), out.size(3), out.size(4))\n",
        "        if isinstance(out.data, torch.cuda.FloatTensor):\n",
        "            zero_pads = zero_pads.cuda()\n",
        "        out = torch.cat([out.data, zero_pads], dim=1)\n",
        "        return out\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
        "        downsample = None\n",
        "        if stride!= 1 or self.in_planes!= planes * block.expansion:\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(self._downsample_basic_block, planes=planes * block.expansion, stride=stride)\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
        "                    nn.BatchNorm3d(planes * block.expansion))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_planes, planes, stride, downsample))\n",
        "        self.in_planes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.in_planes, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        if not self.no_max_pool:\n",
        "            x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def generate_resnet34(**kwargs):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], [64, 128, 256, 512], **kwargs)\n",
        "\n",
        "class ResNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, resnet_model):\n",
        "        super(ResNetFeatureExtractor, self).__init__()\n",
        "        self.features = nn.Sequential(*list(resnet_model.children())[:-1])\n",
        "        self.fc = resnet_model.fc\n",
        "        self.fc_new = nn.Linear(1000, 400) # New FC layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.fc_new(x)\n",
        "        return x\n",
        "\n",
        "# --- VideoMAE imports ---\n",
        "try:\n",
        "    from transformers import VideoMAEModel, AutoImageProcessor\n",
        "except ImportError:\n",
        "    print(\"Transformers library not found. Please run the installation cell first.\")\n",
        "    VideoMAEModel = None\n",
        "    AutoImageProcessor = None\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "def preprocess_subtomogram_resnet(mrc_data, target_shape=(96, 96, 96)):\n",
        "    data = mrc_data.astype(np.float32)\n",
        "    min_val, max_val = data.min(), data.max()\n",
        "    if max_val > min_val:\n",
        "        data = (data - min_val) / (max_val - min_val)\n",
        "    zoom_factors = [t / s for t, s in zip(target_shape, data.shape)]\n",
        "    resized_data = zoom(data, zoom_factors, order=1)\n",
        "    # Replicate the single channel to three channels\n",
        "    replicated_data = np.stack([resized_data]*3, axis=0)\n",
        "    tensor_data = torch.from_numpy(replicated_data).unsqueeze(0) # Add batch dimension\n",
        "    # Normalize each channel individually\n",
        "    normalize = transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    normalized_channels = [normalize(c.unsqueeze(0)) for c in tensor_data.squeeze(0)]\n",
        "    return torch.cat(normalized_channels, dim=0).unsqueeze(0)\n",
        "\n",
        "\n",
        "def preprocess_subtomogram_videomae(mrc_data, image_processor, num_frames=16):\n",
        "    data = mrc_data.astype(np.float32)\n",
        "    min_val, max_val = data.min(), data.max()\n",
        "    if max_val > min_val:\n",
        "        data = 255 * (data - min_val) / (max_val - min_val)\n",
        "    data = data.astype(np.uint8)\n",
        "    depth = data.shape[0]\n",
        "    indices = np.linspace(0, depth - 1, num_frames, dtype=int)\n",
        "    frames = [Image.fromarray(data[i]) for i in indices]\n",
        "    inputs = image_processor(list(frames), return_tensors=\"pt\")\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "D9ykjcRTVykn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34cbd146",
        "outputId": "ff78a3f4-362f-49f3-c970-be9e22c6f0c4"
      },
      "source": [
        "# Cell to download the ResNet weights\n",
        "import requests\n",
        "\n",
        "url = 'https://download.pytorch.org/models/resnet34-333f7ec4.pth'\n",
        "file_path = '/resnet-34-kinetics-cpu.pth'\n",
        "\n",
        "try:\n",
        "    print(f\"Downloading {url} to {file_path}...\")\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(file_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    print(\"Download complete.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading file: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://download.pytorch.org/models/resnet34-333f7ec4.pth to /resnet-34-kinetics-cpu.pth...\n",
            "Download complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the Pipeline\n",
        "\n",
        "# Define file paths\n",
        "data_path = './cryo-ET-samples'\n",
        "resnet_weights_path = '/resnet-34-kinetics-cpu.pth'\n",
        "output_dir = './results'\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "B1XY1TpsYBLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9267c4"
      },
      "source": [
        "# --- Feature Extraction and Analysis Functions ---\n",
        "\n",
        "def extract_features_resnet(model, data_path):\n",
        "    \"\"\"\n",
        "    Extracts features from .mrc files using the 3D-ResNet34 model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    features = []\n",
        "    subtomogram_ids = []\n",
        "    mrc_files = sorted(glob.glob(os.path.join(data_path, '*.mrc')))\n",
        "    print(f\"Found {len(mrc_files)} .mrc files for ResNet processing.\")\n",
        "    with torch.no_grad():\n",
        "        for f in mrc_files:\n",
        "            try:\n",
        "                with mrcfile.open(f, permissive=True) as mrc:\n",
        "                    subtomogram_id = os.path.basename(f).split('.')[0]\n",
        "                    preprocessed_data = preprocess_subtomogram_resnet(mrc.data)\n",
        "                    feature = model(preprocessed_data)\n",
        "                    features.append(feature.cpu().numpy().flatten())\n",
        "                    subtomogram_ids.append(subtomogram_id)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not process file {f}: {e}\")\n",
        "    return np.array(features), subtomogram_ids\n",
        "\n",
        "def extract_features_videomae(model, processor, data_path):\n",
        "    \"\"\"\n",
        "    Extracts features from .mrc files using the VideoMAE model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    features = []\n",
        "    subtomogram_ids = []\n",
        "    mrc_files = sorted(glob.glob(os.path.join(data_path, '*.mrc')))\n",
        "    print(f\"Found {len(mrc_files)} .mrc files for VideoMAE processing.\")\n",
        "    with torch.no_grad():\n",
        "        for f in mrc_files:\n",
        "            try:\n",
        "                with mrcfile.open(f, permissive=True) as mrc:\n",
        "                    subtomogram_id = os.path.basename(f).split('.')[0]\n",
        "                    inputs = preprocess_subtomogram_videomae(mrc.data, processor)\n",
        "                    outputs = model(**inputs)\n",
        "                    feature = outputs.last_hidden_state.mean(dim=1)\n",
        "                    features.append(feature.cpu().numpy().flatten())\n",
        "                    subtomogram_ids.append(subtomogram_id)\n",
        "            except Exception as e:\n",
        "                print(f\"Could not process file {f}: {e}\")\n",
        "    return np.array(features), subtomogram_ids\n",
        "\n",
        "def run_tsne_kmeans_and_plot(features, ids, model_name, output_dir):\n",
        "    \"\"\"\n",
        "    Performs t-SNE and K-Means clustering and plots the results.\n",
        "    \"\"\"\n",
        "    print(f\"Running t-SNE and K-Means for {model_name}...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(features)-1))\n",
        "    tsne_results = tsne.fit_transform(features)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(features)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=clusters, cmap='viridis')\n",
        "    plt.title(f't-SNE visualization of {model_name} features')\n",
        "    plt.xlabel('t-SNE component 1')\n",
        "    plt.ylabel('t-SNE component 2')\n",
        "    plt.legend(handles=scatter.legend_elements()[0], labels=range(4))\n",
        "\n",
        "    # Add annotations\n",
        "    for i, txt in enumerate(ids):\n",
        "        plt.annotate(txt, (tsne_results[i, 0], tsne_results[i, 1]))\n",
        "\n",
        "    plot_path = os.path.join(output_dir, f't_sne_{model_name.lower().replace(\" \", \"_\")}.png')\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "    print(f\"Plot saved to {plot_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "a826858d",
        "outputId": "30d6a012-9a0b-45e1-c8c6-ee4526b02c8f",
        "collapsed": true
      },
      "source": [
        "# --- Part 1: 3D-ResNet34 ---\n",
        "print(\"\\n--- Starting 3D-ResNet34 Analysis ---\")\n",
        "if os.path.exists(resnet_weights_path):\n",
        "    resnet_model = generate_resnet34(n_classes=1000, n_input_channels=3)\n",
        "    print(f\"Loading ResNet weights from {resnet_weights_path}...\")\n",
        "    state_dict = torch.load(resnet_weights_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    # Inflate 2D weights to 3D\n",
        "    for name, param in resnet_model.named_parameters():\n",
        "        if 'conv' in name and len(param.shape) == 5 and name in state_dict:\n",
        "            param.data = state_dict[name].unsqueeze(2).repeat(1, 1, param.shape[2], 1, 1)\n",
        "\n",
        "    resnet_extractor = ResNetFeatureExtractor(resnet_model)\n",
        "    resnet_features, resnet_ids = extract_features_resnet(resnet_extractor, data_path)\n",
        "    if resnet_features.size > 0:\n",
        "        run_tsne_kmeans_and_plot(resnet_features, resnet_ids, \"3D-ResNet34\", output_dir)\n",
        "    else:\n",
        "        print(\"No features were extracted from 3D-ResNet34.\")\n",
        "else:\n",
        "    print(f\"Error: ResNet weights file not found at {resnet_weights_path}. Please upload it.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting 3D-ResNet34 Analysis ---\n",
            "Loading ResNet weights from /resnet-34-kinetics-cpu.pth...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot use ``weights_only=True`` with files saved in the legacy .tar format. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-45-2457622524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresnet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_resnet34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading ResNet weights from {resnet_weights_path}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_weights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Inflate 2D weights to 3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                 return _legacy_load(\n\u001b[0m\u001b[1;32m   1488\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1725\u001b[0m         \u001b[0;31m# only if offset is zero we can attempt the legacy tar file loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mlegacy_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   1595\u001b[0m         ) as tar, mkdtemp() as tmpdir:\n\u001b[1;32m   1596\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpickle_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_weights_only_unpickler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   1598\u001b[0m                     \u001b[0;34m\"Cannot use ``weights_only=True`` with files saved in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m                     \u001b[0;34m\"legacy .tar format. \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mUNSAFE_MESSAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot use ``weights_only=True`` with files saved in the legacy .tar format. In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d3619eb"
      },
      "source": [
        "# --- Part 2: VideoMAE ---\n",
        "print(\"\\n--- Starting VideoMAE Analysis ---\")\n",
        "if VideoMAEModel is not None:\n",
        "    print(\"Loading VideoMAE model and processor from Hugging Face...\")\n",
        "    videomae_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "    videomae_model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
        "    videomae_features, videomae_ids = extract_features_videomae(videomae_model, videomae_processor, data_path)\n",
        "    if videomae_features.size > 0:\n",
        "        run_tsne_kmeans_and_plot(videomae_features, videomae_ids, \"VideoMAE\", output_dir)\n",
        "    else:\n",
        "        print(\"No features were extracted from VideoMAE.\")\n",
        "else:\n",
        "    print(\"Skipping VideoMAE analysis because 'transformers' library is not installed.\")\n",
        "\n",
        "print(\"\\n--- Pipeline Finished ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Output Images\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "resnet_plot_path = './results/t_sne_3d-resnet34.png'\n",
        "videomae_plot_path = './results/t_sne_videomae.png'\n",
        "\n",
        "print(\"--- 3D-ResNet34 Feature Visualization ---\")\n",
        "if os.path.exists(resnet_plot_path):\n",
        "    display(Image(filename=resnet_plot_path))\n",
        "else:\n",
        "    print(\"ResNet plot not found.\")\n",
        "\n",
        "print(\"\\n--- VideoMAE Feature Visualization ---\")\n",
        "if os.path.exists(videomae_plot_path):\n",
        "    display(Image(filename=videomae_plot_path))\n",
        "else:\n",
        "    print(\"VideoMAE plot not found.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MKsrNzo_ZmWK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}